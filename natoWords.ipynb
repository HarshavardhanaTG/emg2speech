{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnnNato\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "from Levenshtein import distance\n",
    "import os\n",
    "\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Proof for Figure 5 and Table 7.\n",
    "\n",
    "Here, we train the model with just 16 articulations of each of the NATO word (total 16 * 26 = 416 articulation in the training set).\n",
    "We test our model on a much larger dataset of 1967 nato word articulations from the RAINBOW and GRANDFATHER passages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert NATO words to phoneme sequences.\"\"\"\n",
    "\n",
    "natoAlphabets = [\n",
    "    \"Alfa\", \"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \n",
    "    \"Foxtrot\", \"Golf\", \"Hotel\", \"India\", \"Juliette\",\n",
    "    \"Kilo\", \"Lima\", \"Mike\", \"November\", \"Oscar\", \n",
    "    \"Papa\", \"Quebec\", \"Romeo\", \"Sierra\", \"Tango\",\n",
    "    \"Uniform\", \"Victor\", \"Whiskey\", \"X-ray\", \"Yankee\",\n",
    "    \"Zulu\"]\n",
    "\n",
    "PHONE_DEF = ['AA', 'AE', 'AH', 'AO', 'AY', 'B', 'CH', 'D', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH',\n",
    " 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'P', 'R', 'S', 'T', 'UW', 'V', 'W', 'Y', 'Z', 'SIL']\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF.index(p)\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "phonemizedAlphabets = []\n",
    "for i in range(len(natoAlphabets)):\n",
    "    alphabet = natoAlphabets[i].strip()\n",
    "    alphabet = re.sub(r'[^a-zA-Z\\- \\']', '', alphabet)\n",
    "    alphabet = alphabet.replace('--', '').lower()\n",
    "    phones = []\n",
    "    for p in g2p(alphabet):\n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p):   \n",
    "            phones.append(p)\n",
    "    phonemizedAlphabets.append(phones)\n",
    "\n",
    "phone2index = []\n",
    "for i in range(len(phonemizedAlphabets)):\n",
    "    current = phonemizedAlphabets[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phone2index.append(phoneID)\n",
    "\n",
    "phonemizedLabels = np.zeros((26, 8))\n",
    "for i in range(26):\n",
    "    phonemizedLabels[i, 0:len(phone2index[i])] = phone2index[i]\n",
    "\n",
    "labelLengths = np.zeros((26))\n",
    "for i in range(len(phone2index)):\n",
    "    labelLengths[i] = len(phone2index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOperation(model,  device, trainLoader, rnnOptimizer, Loss):\n",
    "    model.train()\n",
    "    totalLoss = 0\n",
    "    for inputs, targets, inputLengths, targetLengths in trainLoader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "        \n",
    "        rnnOptimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, inputLengths.cpu())\n",
    "        loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "        loss.backward()\n",
    "        rnnOptimizer.step()\n",
    "\n",
    "        totalLoss += loss.item()\n",
    "        \n",
    "    \n",
    "    return totalLoss / len(trainLoader)\n",
    "\n",
    "\n",
    "def valOperation(model, device, valLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in valLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "\n",
    "    return totalLoss / len(valLoader)\n",
    "\n",
    "\"\"\" Number of NATO aplhabets in training set.\"\"\"\n",
    "\n",
    "numberAlphabets = 26\n",
    "trialsPerAlphabet = 20\n",
    "numberTrials = numberAlphabets * trialsPerAlphabet\n",
    "numberChannels = 22\n",
    "windowLength = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Upload data.\"\"\"\n",
    "\n",
    "subjectNumber = 1\n",
    "subject = \"Subject\" + str(subjectNumber)\n",
    "DATA = np.load(\"DATA/\" + subject + \"/trainSet.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"z-normalize and chunk data.\"\"\"\n",
    "\n",
    "mean = np.mean(DATA, axis = -1)\n",
    "std = np.std(DATA, axis = -1)\n",
    "DATA = (DATA - mean[..., np.newaxis])/(std[..., np.newaxis] + 1e-5)\n",
    "\n",
    "slicedMatrices = np.zeros((numberAlphabets * trialsPerAlphabet, 46, numberChannels, numberChannels))\n",
    "for j in range(numberAlphabets * trialsPerAlphabet):\n",
    "    for i in range(46):\n",
    "        where = i * 150 + 300\n",
    "        start = where - 300\n",
    "        End = where + 450\n",
    "        temp = 1/750 * (DATA[j, :, start:End] @ DATA[j, :, start:End].T)\n",
    "        slicedMatrices[j, i] = 0.9 * temp + 0.1 * np.trace(temp) * np.eye(numberChannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Approximately diagonalize the data.\"\"\"\n",
    "\n",
    "manifoldMean = frechetMean()\n",
    "MEAN = manifoldMean.mean(slicedMatrices.reshape(-1, numberChannels, numberChannels))\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "afterMatrices = np.zeros((numberAlphabets * trialsPerAlphabet, 46, numberChannels, numberChannels))\n",
    "for i in range(numberAlphabets * trialsPerAlphabet):\n",
    "    for j in range(46):\n",
    "        temp = eigenvectors.T @ slicedMatrices[i, j] @ eigenvectors\n",
    "        afterMatrices[i, j] = temp\n",
    "labelsByAlphabet = np.array([[i] * trialsPerAlphabet for i in range(numberAlphabets)]).reshape(numberTrials)\n",
    "\n",
    "Indices =  {}\n",
    "for i in range(numberAlphabets):\n",
    "    Indices[i] = []\n",
    "for i in range(len(labelsByAlphabet)):\n",
    "    Indices[labelsByAlphabet[i]].append(i)\n",
    "\n",
    "covariancesLabels = np.zeros((numberAlphabets, trialsPerAlphabet, 46, numberChannels, numberChannels))\n",
    "for i in range(numberAlphabets):\n",
    "    for j in range(trialsPerAlphabet):\n",
    "        covariancesLabels[i, j] = afterMatrices[Indices[i][j]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"DATA/ckptsNatoWords/eigenVectors\" + str(subjectNumber) + \".npy\", eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.data.shape[1])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAIN-VAL data split.\"\"\"\n",
    "\n",
    "trainFeatures = np.zeros((numberAlphabets * 16, 46, numberChannels, numberChannels))\n",
    "trainLabels = np.zeros((numberAlphabets * 16, 8))\n",
    "trainLabelLengths = np.zeros((numberAlphabets * 16))\n",
    "count = 0\n",
    "for i in range(numberAlphabets):\n",
    "    trainFeatures[count:count + 4] = covariancesLabels[i, :4]\n",
    "    trainFeatures[count + 4:count + 8] = covariancesLabels[i, 5:9]\n",
    "    trainFeatures[count + 8:count + 12] = covariancesLabels[i, 10:14]\n",
    "    trainFeatures[count + 12:count + 16] = covariancesLabels[i, 15:19]\n",
    "    \n",
    "    trainLabels[count:count + 16] = np.tile(phonemizedLabels[i], (16, 1))\n",
    "    trainLabelLengths[count:count + 16] = np.tile(labelLengths[i], (16))\n",
    "\n",
    "    count += 16\n",
    "\n",
    "valFeatures = np.zeros((numberAlphabets * 4, 46, numberChannels, numberChannels))\n",
    "valLabels = np.zeros((numberAlphabets * 4, 8))\n",
    "valLabelLengths = np.zeros((numberAlphabets * 4))\n",
    "count = 0\n",
    "for i in range(numberAlphabets):\n",
    "    valFeatures[count] = covariancesLabels[i,4]\n",
    "    valFeatures[count + 1] = covariancesLabels[i, 9]\n",
    "    valFeatures[count + 2] = covariancesLabels[i, 14]\n",
    "    valFeatures[count + 3] = covariancesLabels[i, 19]\n",
    "\n",
    "    valLabels[count:count + 4] = np.tile(phonemizedLabels[i], (4, 1))\n",
    "    valLabelLengths[count:count + 4] = np.tile(labelLengths[i], (4))\n",
    "\n",
    "    count += 4\n",
    "\n",
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valLabelLengths)\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280121\n"
     ]
    }
   ],
   "source": [
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "\n",
    "model = euclideanRnnNato.RnnNet(33, 23, device, numLayers = 1).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 32, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model.\"\"\"\n",
    "valLOSS = []\n",
    "minLOSS = 100\n",
    "for epoch in range(numberEpochs):\n",
    "    trainLoss = trainOperation(model, device, trainDataloader, rnnOptimizer, lossFunction)\n",
    "    valLoss = valOperation(model, device, valDataloader, lossFunction)\n",
    "    valLOSS.append(valLoss)\n",
    "    if minLOSS > valLoss:\n",
    "        minLOSS = valLoss\n",
    "    torch.save(model.state_dict(), \"ckpts/natoEuclidean/\" + str(epoch) + \".pt\")\n",
    "    print(f'Epoch: {epoch + 1}/{numberEpochs}, Training loss: {trainLoss:.4f}, Val loss: {valLoss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ckpts/natoEuclidean/valLoss.npy\", valLOSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
