{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnn\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof for table 1, figure 2, and figure 3. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train LARGE-VOCAB EMG-to-phoneme conversion.\n",
    "\n",
    "For description of the data, please see largeVocabDataVisualization.ipynb\n",
    "\n",
    "Unlike data SMALL-VOCAB, there are no timestamps between words within a sentence. \n",
    "\n",
    "Given a sentence, you decode it fully using CTC loss. The pipeline resembles standard speech-to-text (ASR) techniques.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"https://pypi.org/project/Levenshtein/ - install this Lev distance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Open Data.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"DATA/dataLargeVocab.pkl\", \"rb\") as file:\n",
    "    DATA = pickle.load(file)\n",
    "\n",
    "with open(\"DATA/labelsLargeVocab.pkl\", \"rb\") as file:\n",
    "    LABELS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "English phoneme definitions.\n",
    "\"\"\"\n",
    "tok2id = {}\n",
    "with open(\"DATA/ckptsLargeVocab/lang_phone/tokens.txt\") as f:\n",
    "    for line in f:\n",
    "        s, i = line.strip().split()\n",
    "        i = int(i)\n",
    "        if s == \"<eps>\" or s.startswith(\"#\"):\n",
    "            continue\n",
    "        tok2id[s] = i\n",
    "PHONE_DEF = tok2id\n",
    "\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF[p]\n",
    "\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phonemize the sentences.\n",
    "\"\"\"\n",
    "\n",
    "phonemizedSentences = []\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    phones = []\n",
    "    for p in g2p(LABELS[i]): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p): \n",
    "            phones.append(p)\n",
    "    phonemizedSentences.append(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert phone-to-indices using look-up dictionary PHONE_DEF.\n",
    "\"\"\"\n",
    "\n",
    "phoneIndexedSentences = []\n",
    "for i in range(len(phonemizedSentences)):\n",
    "    current = phonemizedSentences[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phoneIndexedSentences.append(phoneID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenIdToClassIdx(tokenId: int) -> int:\n",
    "    return tokenId - 1   \n",
    "\n",
    "def phoneSeqToClassIdxSeq(phoneSeq):\n",
    "    return [tokenIdToClassIdx(PHONE_DEF[p]) for p in phoneSeq]\n",
    "\n",
    "classIndexedSentences = [phoneSeqToClassIdxSeq(seq) for seq in phonemizedSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pad the phone transcribed sentences to a common length (to be used with CTC loss).\n",
    "\"\"\"\n",
    "\n",
    "phonemizedLabels = np.zeros((len(classIndexedSentences), 76)) - 1\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    phonemizedLabels[i, 0:len(classIndexedSentences[i])] = classIndexedSentences[i]\n",
    "\n",
    "labelLengths = np.zeros((len(classIndexedSentences)))\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    labelLengths[i] = len(classIndexedSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z-normalize the data along the time dimension.\n",
    "\"\"\"\n",
    "\n",
    "normDATA = []\n",
    "for i in range(len(DATA)):\n",
    "    Mean = np.mean(DATA[i], axis = -1)\n",
    "    Std = np.std(DATA[i], axis = -1)\n",
    "    normDATA.append((DATA[i] - Mean[..., np.newaxis])/Std[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Slice the matrices into 50ms segments with a step size of 20ms.\n",
    "\"\"\"\n",
    "\n",
    "slicedMatrices = []\n",
    "for j in range(len(normDATA)):\n",
    "    collect = []\n",
    "    stepSize = 100 \n",
    "    windowSize = 125\n",
    "    dataLength = normDATA[j].shape[1]\n",
    "    numIters = (dataLength - windowSize) // stepSize + 1\n",
    "       \n",
    "    for i in range(numIters):\n",
    "        where = i * stepSize + windowSize\n",
    "        start = where - windowSize\n",
    "        End = where + windowSize\n",
    "        temp = 1/(2 * windowSize) * (normDATA[j][:, start:End] @ normDATA[j][:, start:End].T)\n",
    "        collect.append(0.9 * temp + 0.1 * np.trace(temp) * np.eye(31))\n",
    "    slicedMatrices.append(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diag = TRUE or FALSE. Raw SPD matrices or approximately diagonalized?\"\"\"\n",
    "DIAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matricesForMean = []\n",
    "for i in range(9000):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        matricesForMean.append(slicedMatrices[i][j])\n",
    "\n",
    "matricesForMean = np.array(matricesForMean)\n",
    "manifoldMean = frechetMean()\n",
    "\n",
    "MEAN = manifoldMean.mean(matricesForMean.reshape(-1, 31, 31))\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "identityMatrix = np.eye(31)\n",
    "afterMatrices = np.tile(identityMatrix, (len(slicedMatrices), 409, 1, 1)) \n",
    "inputLengths = np.zeros((len(slicedMatrices)))\n",
    "for i in range(len(slicedMatrices)):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        if DIAG:\n",
    "            temp = eigenvectors.T @ slicedMatrices[i][j] @ eigenvectors\n",
    "        else:\n",
    "            temp = slicedMatrices[i][j]\n",
    "        afterMatrices[i, j] = temp\n",
    "    inputLengths[i] = len(slicedMatrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, inputLength, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "        self.inputLength = inputLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.inputLength[index])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train-validation-test split.\n",
    "\"\"\"\n",
    "\n",
    "trainFeatures = afterMatrices[:8000]\n",
    "trainLabels = phonemizedLabels[:8000]\n",
    "trainLabelLengths = labelLengths[:8000]\n",
    "trainInputLengths = inputLengths[:8000]\n",
    "\n",
    "valFeatures = afterMatrices[8000:9000]\n",
    "valLabels = phonemizedLabels[8000:9000]\n",
    "valLabelLengths = labelLengths[8000:9000]\n",
    "valInputLengths = inputLengths[8000:9000]\n",
    "\n",
    "testFeatures = afterMatrices[9000:]\n",
    "testLabels = phonemizedLabels[9000:]\n",
    "testLabelLengths = labelLengths[9000:]\n",
    "testInputLengths = inputLengths[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainInputLengths, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valInputLengths, valLabelLengths)\n",
    "testDataset = BaseDataset(testFeatures, testLabels, testInputLengths, testLabelLengths)\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)\n",
    "testDataloader = DataLoader(testDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348591\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To replicate the PER (phoneme error rate) for various model sizes and layers, change the variable here:\n",
    "euclideanRnn.RnnNet(41, modelHiddenDimension = 25, device, numLayers = 3).to(device)\n",
    "\"\"\"\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "model = euclideanRnn.RnnNet(41, 25, device, numLayers = 3).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 40, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOperation(model, device, testLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    Outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in testLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "\n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "            Outputs.append(outputs.transpose(0, 1))\n",
    "\n",
    "    return Outputs, totalLoss / len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple beam-search algorithm.\n",
    "\"\"\"\n",
    "\n",
    "def ctcPrefixBeamSearch(\n",
    "    logProbs,\n",
    "    testLen = None,\n",
    "    beamSize = 50,\n",
    "    blank = 40,\n",
    "    topk = None,\n",
    "    allowDoubles = True,\n",
    "):\n",
    "    \n",
    "    lp = np.asarray(logProbs)\n",
    "    Ttotal, V = lp.shape\n",
    "    T = Ttotal if testLen is None else int(min(testLen, Ttotal))\n",
    "\n",
    "    beams = {(): (0.0, -np.inf)}\n",
    "\n",
    "    def add(store, seq, addPb, addPnb):\n",
    "        if seq in store:\n",
    "            pb, pnb = store[seq]\n",
    "            if addPb  != -np.inf: pb  = np.logaddexp(pb,  addPb)\n",
    "            if addPnb != -np.inf: pnb = np.logaddexp(pnb, addPnb)\n",
    "            store[seq] = (pb, pnb)\n",
    "        else:\n",
    "            store[seq] = (addPb, addPnb)\n",
    "\n",
    "    for t in range(T):\n",
    "        row = lp[t] \n",
    "        new = {}\n",
    "\n",
    "        if topk is not None and topk < V:\n",
    "            cand = np.argpartition(row, -topk)[-topk:]\n",
    "            if blank not in cand:\n",
    "                worstIdx = cand[np.argmin(row[cand])]\n",
    "                cand[cand == worstIdx] = blank\n",
    "        else:\n",
    "            cand = range(V)\n",
    "\n",
    "        for seq, (pb, pnb) in beams.items():\n",
    "            add(new, seq, np.logaddexp(pb, pnb) + row[blank], -np.inf)\n",
    "\n",
    "            last = seq[-1] if seq else None\n",
    "\n",
    "            for c in cand:\n",
    "                if c == blank:\n",
    "                    continue\n",
    "                pC = row[c]\n",
    "\n",
    "                if c == last:\n",
    "            \n",
    "                    add(new, seq, -np.inf, pnb + pC)\n",
    "\n",
    "                    if allowDoubles:\n",
    "                        add(new, seq + (c,), -np.inf, pb + pC)\n",
    "                else:\n",
    "                    add(new, seq + (c,), -np.inf, np.logaddexp(pb, pnb) + pC)\n",
    "\n",
    "        if len(new) > beamSize:\n",
    "            items = sorted(new.items(),\n",
    "                           key = lambda kv: np.logaddexp(*kv[1]),\n",
    "                           reverse = True)[:beamSize]\n",
    "            beams = dict(items)\n",
    "        else:\n",
    "            beams = new\n",
    "\n",
    "    bestSeq = max(beams.items(), key = lambda kv: np.logaddexp(*kv[1]))[0]\n",
    "    return bestSeq\n",
    "\n",
    "def findClosestTranscription(decodedTranscript, phoneticTranscription):\n",
    "    \n",
    "    dist = Levenshtein.distance(decodedTranscript, phoneticTranscription)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS:  1.9241562139603399\n"
     ]
    }
   ],
   "source": [
    "modelWeight = torch.load(\"DATA/ckptsLargeVocab/ckptWithoutSpaces.pt\", weights_only = True)\n",
    "model.load_state_dict(modelWeight)\n",
    "output, testLoss = testOperation(model, device, testDataloader, lossFunction)\n",
    "\n",
    "print(\"TEST LOSS: \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for o in output:\n",
    "    for oo in o:\n",
    "        outs.append(oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONE_DEF1 = {}\n",
    "for k, v in PHONE_DEF.items():\n",
    "    PHONE_DEF1[v - 1] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVS = []\n",
    "decodedOut = []\n",
    "for i in range(1970):\n",
    "    decodedSymbols = ctcPrefixBeamSearch(outs[i].cpu().numpy(), testInputLengths[i]) \n",
    "    phoneOut = []\n",
    "    for i in range(len(decodedSymbols)):\n",
    "        phoneOut.append(PHONE_DEF1[decodedSymbols[i]])\n",
    "    decodedOut.append(phoneOut)\n",
    "\n",
    "levs = []\n",
    "phoneLENGTHS = []\n",
    "for i in range(len(decodedOut)):\n",
    "    phoneLENGTHS.append(len(phonemizedSentences[9000 + i]))\n",
    "    levs.append(findClosestTranscription(decodedOut[i], phonemizedSentences[9000 + i]))\n",
    "LEVS.append(np.mean(levs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sentences:  19.590862944162435\n",
      "Mean phoneme error rate (insertion errors + deletion errors + substitution errors):  9.93502538071066\n",
      "Percent phoneme error:  0.5071254599160492\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of sentences: \", np.mean(phoneLENGTHS))\n",
    "print(\"Mean phoneme error rate (insertion errors + deletion errors + substitution errors): \", np.mean(levs))\n",
    "print(\"Percent phoneme error: \", np.mean(levs)/np.mean(phoneLENGTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 991 1341 1562 1543 1381 1687 1214  452 1920  693  841  685  214 1619\n",
      "  190 1180  540  669 1055 1436 1798 1030  602 1756   33 1872 1771  361\n",
      "  682  192 1222  633  850  852  976  881  564  132  762  313  155  620\n",
      " 1473  973   34 1855  202 1478   59  839  989  215 1150  225 1119 1892\n",
      "   98  522  939  563  288 1184 1185 1035 1315 1751 1962  818   94 1633\n",
      " 1901  771 1542 1467 1924 1135 1326 1123  454 1059  417  536  784  637\n",
      "  671 1927  974  775 1879 1002 1179  924  996 1800  354  449 1056  302\n",
      " 1769 1081]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sort the decoded sentences from best-to-worst. Display 100 best decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['DH', 'EY', 'V', 'G', 'AA', 'T', 'AH', 'N', 'AY', 'S', 'W', 'AH', 'N']\n",
      "Ground truth phoneme sequence:  ['DH', 'EY', 'V', 'G', 'AA', 'T', 'AH', 'N', 'AY', 'S', 'W', 'AH', 'N']\n",
      "Ground truth label:  theyve got a nice one\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  0\n",
      "Length of ground truth sequence:  13\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "which = 991\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[9000 + which])\n",
    "print(\"Ground truth label: \", LABELS[9000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[9000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[9000 + which]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ICEFALL_ROOT = Path(\"/mnt/dataDrive/emgFullCorpora/toUpload/Icefall/\").resolve()\n",
    "if str(ICEFALL_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ICEFALL_ROOT))\n",
    "\n",
    "\n",
    "from icefall.lexicon import Lexicon\n",
    "from icefall.decode import get_lattice, one_best_decoding\n",
    "from icefall.utils import get_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HLG is created using the following steps.\n",
    "We use LibriSpeech-100 ranscriptions to create the graph. It has approximalety 38000 sentences and 35000 unique words. \n",
    "\n",
    "1) python prepare_lang.py --lang-dir DATA/ckptsLargeVocab/lang_phone --debug false (this expects lexicon.txt)\n",
    "\n",
    "2a) /mnt/dataDrive/kenLM/kenlm/build/bin/lmplz -o 4   < DATA/ckptsLargeVocab/libri100.txt > DATA/ckptsLargeVocab/lm/4gram.arpa\n",
    " b)  python3 -m kaldilm   --read-symbol-table DATA/ckptsLargeVocab/lang_phone/words.txt   --disambig-symbol '#0' --max-order 4   DATA/ckptsLargeVocab/lm/4gram.arpa > DATA/ckptsLargeVocab/lm/G_4_gram.fst.txt\n",
    "\n",
    "3) python prepare_lang_fst.py   --lang-dir DATA/ckptsLargeVocab/lang_phone   --has-silence 1   --ngram-G DATA/ckptsLargeVocab/lm/G_4_gram.fst.txt\n",
    "4) python compile_hlg.py   --lang-dir DATA/ckptsLargeVocab/lang_phone   --lm G_4_gram\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "langDir = \"DATA/ckptsLargeVocab/lang_phone\"\n",
    "device = \"cpu\"  \n",
    "\n",
    "\n",
    "def readSymbolTable(path):\n",
    "    s2i, i2s, maxId = {}, {}, -1\n",
    "    with open(path, \"r\", encoding = \"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            s, i = line.split()\n",
    "            i = int(i)\n",
    "            s2i[s] = i\n",
    "            i2s[i] = s\n",
    "            if i > maxId:\n",
    "                maxId = i\n",
    "    return s2i, i2s, maxId + 1 \n",
    "\n",
    "def safeLoadFsaDict(ptPath):\n",
    "    try:\n",
    "        d = torch.load(ptPath, map_location = \"cpu\", weights_only = True)\n",
    "    except TypeError:\n",
    "        d = torch.load(ptPath, map_location = \"cpu\")\n",
    "    except Exception:\n",
    "        d = torch.load(ptPath, map_location = \"cpu\", weights_only = False)\n",
    "    return d\n",
    "\n",
    "def buildAcousticTokenIds(tok2id, VMinus1):\n",
    "    EXCLUDE = {\n",
    "        \"<eps>\", \"<blk>\",\n",
    "        \"SPN\", \"NSN\", \"<SPOKEN_NOISE>\", \"<UNK>\", \"<NOISE>\", \"<SIL>\", \"<NOISE_SIG>\"\n",
    "    }\n",
    "    items = [(sym, tid) for sym, tid in tok2id.items()\n",
    "             if sym not in EXCLUDE and not sym.startswith(\"#\") and sym != \"<eps>\"]\n",
    "\n",
    "    items.sort(key = lambda x: x[1])\n",
    "\n",
    "    if len(items) < VMinus1:\n",
    "        found = \", \".join(sym for sym, _ in items)\n",
    "        raise RuntimeError(\n",
    "            f\"Found only {len(items)} acoustic tokens, need {VMinus1}.\\n\"\n",
    "            f\"Found: {found}\\n\"\n",
    "            f\"Tip: If your training used a different phone inventory, add those symbols back.\"\n",
    "        )\n",
    "    items = items[:VMinus1]\n",
    "    return [tid for _, tid in items]\n",
    "\n",
    "\n",
    "\n",
    "langPath = Path(langDir)\n",
    "assert langPath.exists(), f\"Not found: {langPath}\"\n",
    "\n",
    "tok2id, id2tok, CTok = readSymbolTable(langPath / \"tokens.txt\")\n",
    "w2id, id2word, CWord = readSymbolTable(langPath / \"words.txt\")\n",
    "\n",
    "lex = Lexicon(langDir)\n",
    "\n",
    "HLGDict = safeLoadFsaDict(langPath / \"HLG.pt\")\n",
    "HLG = k2.Fsa.from_dict(HLGDict)\n",
    "HLG = k2.arc_sort(HLG).to(device).requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODED_WORDS = []\n",
    "for i in range(len(outs)):\n",
    "    logitsORlogprobsTV = torch.as_tensor(outs[i][:int(testInputLengths[i]), :], dtype = torch.float32)\n",
    "    assert logitsORlogprobsTV.ndim == 2, logitsORlogprobsTV.shape\n",
    "    T, V = logitsORlogprobsTV.shape\n",
    "    assert V >= 2, \"Expect at least 2 classes (phones + blank).\"\n",
    "    BLANK_CLASS = V - 1  \n",
    "\n",
    "    nnetOutput = logitsORlogprobsTV.unsqueeze(0).to(device)\n",
    "\n",
    "    acousticTokenIds = buildAcousticTokenIds(tok2id, VMinus1 = V-1)\n",
    "    assert len(acousticTokenIds) == V-1\n",
    "\n",
    "    \n",
    "    NEG_INF = -1e30\n",
    "    N = nnetOutput.shape[0]\n",
    "    nnetLogitsToken = torch.full(\n",
    "        (N, T, CTok), NEG_INF, dtype = nnetOutput.dtype, device = nnetOutput.device\n",
    "    )\n",
    "\n",
    "    nnetLogitsToken[..., 0] = nnetOutput[..., BLANK_CLASS]\n",
    "\n",
    "    for clsIdx, tokId in enumerate(acousticTokenIds):\n",
    "        nnetLogitsToken[..., tokId] = nnetOutput[..., clsIdx]\n",
    "\n",
    "    nnetLogprobsToken = nnetLogitsToken\n",
    "    supervisionSegments = torch.tensor([[0, 0, T]], dtype = torch.int32, device = \"cpu\")\n",
    "    HLG = HLG.to(nnetLogprobsToken.device)\n",
    "    \n",
    "    lattice = get_lattice(\n",
    "    nnet_output = nnetLogprobsToken,\n",
    "    decoding_graph = HLG,\n",
    "    supervision_segments = supervisionSegments,\n",
    "    search_beam = 50,\n",
    "    output_beam = 50,\n",
    "    min_active_states = 30,\n",
    "    max_active_states = 10000)\n",
    "\n",
    "    best = one_best_decoding(lattice, use_double_scores = True)\n",
    "    idsList = get_texts(best) \n",
    "    decodedWords = [id2word[i] for i in idsList[0] if i in id2word]\n",
    "    DECODED_WORDS.append(decodedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "WER = []\n",
    "for i in range(len(DECODED_WORDS)):\n",
    "    ref = LABELS[9000 + i]\n",
    "    hyp = \" \".join(DECODED_WORDS[i])\n",
    "\n",
    "    werScore = jiwer.wer(ref, hyp)\n",
    "    WER.append(werScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735261408078667\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(WER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 991 1341 1562 1543 1381 1687 1214  452 1920  693  841  685  214 1619\n",
      "  190 1180  540  669 1055 1436 1798 1030  602 1756   33 1872 1771  361\n",
      "  682  192 1222  633  850  852  976  881  564  132  762  313  155  620\n",
      " 1473  973   34 1855  202 1478   59  839  989  215 1150  225 1119 1892\n",
      "   98  522  939  563  288 1184 1185 1035 1315 1751 1962  818   94 1633\n",
      " 1901  771 1542 1467 1924 1135 1326 1123  454 1059  417  536  784  637\n",
      "  671 1927  974  775 1879 1002 1179  924  996 1800  354  449 1056  302\n",
      " 1769 1081]\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theyve got us one\n",
      "theyve got a nice one\n"
     ]
    }
   ],
   "source": [
    "which = 991\n",
    "print(\" \".join(DECODED_WORDS[which]))\n",
    "print(LABELS[9000 + which])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
