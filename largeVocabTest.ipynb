{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnn\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof for table 1, figure 2, and figure 3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train LARGE-VOCAB EMG-to-phoneme conversion.\n",
    "\n",
    "For description of the data, please see largeVocabDataVisualization.ipynb\n",
    "\n",
    "Unlike data SMALL-VOCAB, there are no timestamps between words within a sentence. \n",
    "\n",
    "Given a sentence, you decode it fully using CTC loss. The pipeline resembles standard speech-to-text (ASR) techniques.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"https://pypi.org/project/Levenshtein/ - install this Lev distance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Open Data.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"DATA/dataLargeVocab.pkl\", \"rb\") as file:\n",
    "    DATA = pickle.load(file)\n",
    "\n",
    "with open(\"DATA/labelsLargeVocab.pkl\", \"rb\") as file:\n",
    "    LABELS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "English phoneme definitions.\n",
    "\"\"\"\n",
    "\n",
    "PHONE_DEF = ['AO', 'OY', 'DH', 'ZH', 'SH', 'CH', 'UH', 'NG', 'IY', 'AA', 'W', 'S', 'IH', 'K', 'EY', 'JH', 'Y', 'N', 'OW', 'M', 'P', 'T', 'B', 'AY', 'UW', 'R', 'G', 'EH', 'Z', 'TH', 'AW', \n",
    "             'HH', 'AH', 'AE', 'L', 'ER', 'F', 'V', 'D', ' ', 'SIL']\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF.index(p)\n",
    "\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phonemize the sentences.\n",
    "\"\"\"\n",
    "\n",
    "phonemizedSentences = []\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    phones = []\n",
    "    for p in g2p(LABELS[i]): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p) or p == \" \": \n",
    "            phones.append(p)\n",
    "    phonemizedSentences.append(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert phone-to-indices using look-up dictionary PHONE_DEF.\n",
    "\"\"\"\n",
    "\n",
    "phoneIndexedSentences = []\n",
    "for i in range(len(phonemizedSentences)):\n",
    "    current = phonemizedSentences[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phoneIndexedSentences.append(phoneID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pad the phone transcribed sentences to a common length (to be used with CTC loss).\n",
    "\"\"\"\n",
    "\n",
    "phonemizedLabels = np.zeros((len(phoneIndexedSentences), 76)) - 1\n",
    "for i in range(len(phoneIndexedSentences)):\n",
    "    phonemizedLabels[i, 0:len(phoneIndexedSentences[i])] = phoneIndexedSentences[i]\n",
    "\n",
    "labelLengths = np.zeros((len(phoneIndexedSentences)))\n",
    "for i in range(len(phoneIndexedSentences)):\n",
    "    labelLengths[i] = len(phoneIndexedSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z-normalize the data along the time dimension.\n",
    "\"\"\"\n",
    "\n",
    "normDATA = []\n",
    "for i in range(len(DATA)):\n",
    "    Mean = np.mean(DATA[i], axis = -1)\n",
    "    Std = np.std(DATA[i], axis = -1)\n",
    "    normDATA.append((DATA[i] - Mean[..., np.newaxis])/Std[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Slice the matrices into 50ms segments with a step size of 20ms.\n",
    "\"\"\"\n",
    "\n",
    "slicedMatrices = []\n",
    "for j in range(len(normDATA)):\n",
    "    collect = []\n",
    "    stepSize = 100 \n",
    "    windowSize = 125\n",
    "    dataLength = normDATA[j].shape[1]\n",
    "    numIters = (dataLength - windowSize) // stepSize + 1\n",
    "       \n",
    "    for i in range(numIters):\n",
    "        where = i * stepSize + windowSize\n",
    "        start = where - windowSize\n",
    "        End = where + windowSize\n",
    "        temp = 1/(2 * windowSize) * (normDATA[j][:, start:End] @ normDATA[j][:, start:End].T)\n",
    "        collect.append(0.9 * temp + 0.1 * np.trace(temp) * np.eye(31))\n",
    "    slicedMatrices.append(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diag = TRUE or FALSE. Raw SPD matrices or approximately diagonalized?\"\"\"\n",
    "DIAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Approximately diagonalize the matrices using Frechet mean.\n",
    "\"\"\"\n",
    "\n",
    "MEAN = np.load(\"DATA/ckptsLargeVocab/frechetMeanLargeVocab.npy\")\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "identityMatrix = np.eye(31)\n",
    "afterMatrices = np.tile(identityMatrix, (len(slicedMatrices), 409, 1, 1)) \n",
    "inputLengths = np.zeros((len(slicedMatrices)))\n",
    "for i in range(len(slicedMatrices)):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        if DIAG:\n",
    "            temp = eigenvectors.T @ slicedMatrices[i][j] @ eigenvectors\n",
    "        else: \n",
    "            temp = slicedMatrices[i][j]\n",
    "        afterMatrices[i, j] = temp\n",
    "    inputLengths[i] = len(slicedMatrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, inputLength, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "        self.inputLength = inputLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.inputLength[index])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train-validation-test split.\n",
    "\"\"\"\n",
    "\n",
    "trainFeatures = afterMatrices[:8000]\n",
    "trainLabels = phonemizedLabels[:8000]\n",
    "trainLabelLengths = labelLengths[:8000]\n",
    "trainInputLengths = inputLengths[:8000]\n",
    "\n",
    "valFeatures = afterMatrices[8000:9000]\n",
    "valLabels = phonemizedLabels[8000:9000]\n",
    "valLabelLengths = labelLengths[8000:9000]\n",
    "valInputLengths = inputLengths[8000:9000]\n",
    "\n",
    "testFeatures = afterMatrices[9000:]\n",
    "testLabels = phonemizedLabels[9000:]\n",
    "testLabelLengths = labelLengths[9000:]\n",
    "testInputLengths = inputLengths[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainInputLengths, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valInputLengths, valLabelLengths)\n",
    "testDataset = BaseDataset(testFeatures, testLabels, testInputLengths, testLabelLengths)\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)\n",
    "testDataloader = DataLoader(testDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348591\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To replicate the PER (phoneme error rate) for various model sizes and layers, change the variable here:\n",
    "euclideanRnn.RnnNet(41, modelHiddenDimension = 25, device, numLayers = 3).to(device)\n",
    "\"\"\"\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "model = euclideanRnn.RnnNet(41, 25, device, numLayers = 3).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 40, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOperation(model, device, testLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    Outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in testLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "\n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "            Outputs.append(outputs.transpose(0, 1))\n",
    "\n",
    "    return Outputs, totalLoss / len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple beam-search algorithm.\n",
    "\"\"\"\n",
    "\n",
    "def ctcPrefixBeamSearch(\n",
    "    logProbs,\n",
    "    testLen = None,\n",
    "    beamSize = 5,\n",
    "    blank = 40,\n",
    "    topk = None,\n",
    "    allowDoubles = True,\n",
    "):\n",
    "    \n",
    "    lp = np.asarray(logProbs)\n",
    "    Ttotal, V = lp.shape\n",
    "    T = Ttotal if testLen is None else int(min(testLen, Ttotal))\n",
    "\n",
    "    beams = {(): (0.0, -np.inf)}\n",
    "\n",
    "    def add(store, seq, addPb, addPnb):\n",
    "        if seq in store:\n",
    "            pb, pnb = store[seq]\n",
    "            if addPb  != -np.inf: pb  = np.logaddexp(pb,  addPb)\n",
    "            if addPnb != -np.inf: pnb = np.logaddexp(pnb, addPnb)\n",
    "            store[seq] = (pb, pnb)\n",
    "        else:\n",
    "            store[seq] = (addPb, addPnb)\n",
    "\n",
    "    for t in range(T):\n",
    "        row = lp[t] \n",
    "        new = {}\n",
    "\n",
    "        if topk is not None and topk < V:\n",
    "            cand = np.argpartition(row, -topk)[-topk:]\n",
    "            if blank not in cand:\n",
    "                worstIdx = cand[np.argmin(row[cand])]\n",
    "                cand[cand == worstIdx] = blank\n",
    "        else:\n",
    "            cand = range(V)\n",
    "\n",
    "        for seq, (pb, pnb) in beams.items():\n",
    "            add(new, seq, np.logaddexp(pb, pnb) + row[blank], -np.inf)\n",
    "\n",
    "            last = seq[-1] if seq else None\n",
    "\n",
    "            for c in cand:\n",
    "                if c == blank:\n",
    "                    continue\n",
    "                pC = row[c]\n",
    "\n",
    "                if c == last:\n",
    "            \n",
    "                    add(new, seq, -np.inf, pnb + pC)\n",
    "\n",
    "                    if allowDoubles:\n",
    "                        add(new, seq + (c,), -np.inf, pb + pC)\n",
    "                else:\n",
    "                    add(new, seq + (c,), -np.inf, np.logaddexp(pb, pnb) + pC)\n",
    "\n",
    "        if len(new) > beamSize:\n",
    "            items = sorted(new.items(),\n",
    "                           key = lambda kv: np.logaddexp(*kv[1]),\n",
    "                           reverse = True)[:beamSize]\n",
    "            beams = dict(items)\n",
    "        else:\n",
    "            beams = new\n",
    "\n",
    "    bestSeq = max(beams.items(), key = lambda kv: np.logaddexp(*kv[1]))[0]\n",
    "    return bestSeq\n",
    "\n",
    "def findClosestTranscription(decodedTranscript, phoneticTranscription):\n",
    "    \n",
    "    dist = Levenshtein.distance(decodedTranscript, phoneticTranscription)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS:  1.8337221510948674\n"
     ]
    }
   ],
   "source": [
    "modelWeight = torch.load(\"DATA/ckptsLargeVocab/ckptLargeVocab.pt\", weights_only = True)\n",
    "model.load_state_dict(modelWeight)\n",
    "output, testLoss = testOperation(model, device, testDataloader, lossFunction)\n",
    "\n",
    "print(\"TEST LOSS: \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for o in output:\n",
    "    for oo in o:\n",
    "        outs.append(oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVS = []\n",
    "decodedOut = []\n",
    "for i in range(1970):\n",
    "    decodedSymbols = ctcPrefixBeamSearch(outs[i].cpu().numpy(), testInputLengths[i]) \n",
    "    phoneOut = []\n",
    "    for i in range(len(decodedSymbols)):\n",
    "        phoneOut.append(PHONE_DEF[decodedSymbols[i]])\n",
    "    decodedOut.append(phoneOut)\n",
    "\n",
    "levs = []\n",
    "phoneLENGTHS = []\n",
    "for i in range(len(decodedOut)):\n",
    "    phoneLENGTHS.append(len(phonemizedSentences[9000 + i]))\n",
    "    levs.append(findClosestTranscription(decodedOut[i], phonemizedSentences[9000 + i]))\n",
    "LEVS.append(np.mean(levs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sentences:  24.543654822335025\n",
      "Mean phoneme error rate (insertion errors + deletion errors + substitution errors):  11.741116751269036\n",
      "Percent phoneme error:  0.47837686914438177\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of sentences: \", np.mean(phoneLENGTHS))\n",
    "print(\"Mean phoneme error rate (insertion errors + deletion errors + substitution errors): \", np.mean(levs))\n",
    "print(\"Percent phoneme error: \", np.mean(levs)/np.mean(phoneLENGTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1381  170 1800  939 1525 1920 1056 1185  988  669  835  741  536 1214\n",
      "  989 1128 1546   42  154  892  810 1548 1538 1126  779 1908 1130 1892\n",
      " 1447  563   55 1751   59 1035 1121   89  374  237  161 1771 1119 1901\n",
      "  522 1027 1768 1373 1686  785  155 1898  790  239 1039   87 1821 1037\n",
      "  602 1855  188 1467  182 1116  894  740 1047  806  408 1244  486  289\n",
      "  452 1817 1687 1598  738 1550  111  119  706  841 1504  764 1437  897\n",
      "  197 1633  639  236 1478 1212 1620 1798 1217 1797 1055  655  787  428\n",
      "  192 1315]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sort the decoded sentences from best-to-worst. Display 100 best decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['IH', 'T', ' ', 'W', 'AA', 'Z', ' ', 'P', 'EY', 'T', ' ', 'F', 'AO', 'R']\n",
      "Ground truth phoneme sequence:  ['IH', 'T', ' ', 'W', 'AA', 'Z', ' ', 'P', 'EY', 'D', ' ', 'F', 'AO', 'R']\n",
      "Ground truth label:  it was paid for\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  1\n",
      "Length of ground truth sequence:  14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "which = 1381\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[9000 + which])\n",
    "print(\"Ground truth label: \", LABELS[9000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[9000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[9000 + which]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do word modeling. It is a very simple 3-gram model.\n",
    "The model was trained on the first 9000 sentences in the dataset (i.e., train and validation sets) with kneLM using the command: lmplz -o 3 < train.txt > 3gram.arpa\n",
    "\n",
    "1) We mainly rely on matching phoneme segments to word pronunciations in a lexicon dectionary containing all possible unique words.\n",
    "2) kneLM language model is only used to disambiguate and choose coherent word sequences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from itertools import product\n",
    "from heapq import heappush, heappop\n",
    "from jiwer import wer\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /mnt/dataDrive/emgFullCorpora/toUpload/DATA/ckptsLargeVocab/3gramLibri.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lm = kenlm.Model(\"DATA/ckptsLargeVocab/3gramLibri.arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/ckptsLargeVocab/libri100.txt\", \"r\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "\n",
    "uniqueWords = set()\n",
    "for line in lines:\n",
    "    words = re.findall(r\"\\b\\w+\\b\", line.lower())\n",
    "    uniqueWords.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34541\n"
     ]
    }
   ],
   "source": [
    "print(len(uniqueWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37663\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemizedWords = {}\n",
    "\n",
    "for word in sorted(uniqueWords):\n",
    "    phones = []\n",
    "    for p in g2p(word): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p) or p == \" \": \n",
    "            phones.append(p)\n",
    "    phonemizedWords[word] = phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzyMatch(\n",
    "    phonemeSegment: List[str],\n",
    "    lexicon: Dict[str, List[str]],\n",
    "    maxDist: int = 8,\n",
    "    topK: int = 20\n",
    ") -> List[Tuple[str, float]]:\n",
    "    matches = []\n",
    "    for word, phonemes in lexicon.items():\n",
    "        dist = Levenshtein.distance(phonemeSegment, phonemes)\n",
    "        if dist <= maxDist:\n",
    "            normDist = dist / max(len(phonemeSegment), len(phonemes))\n",
    "            matches.append((word, normDist))\n",
    "    return sorted(matches, key=lambda x: x[1])[:topK]\n",
    "\n",
    "def phoneme2wordsHardSegmentation(\n",
    "    phonemes: List[str],\n",
    "    lexicon: Dict[str, List[str]],\n",
    "    lm,\n",
    "    blankToken: str = ' ',\n",
    "    lambdaLM: float = 0.17,\n",
    "    lambdaDist: float = 0.83,\n",
    "    minLen: int = 2\n",
    ") -> Tuple[List[str], float]:\n",
    "   \n",
    "    segments = []\n",
    "    buffer = []\n",
    "    for p in phonemes:\n",
    "        if p == blankToken:\n",
    "            if len(buffer) >= minLen or buffer == [\"AH\"] or buffer == ['AY']:\n",
    "                segments.append(buffer)\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(p)\n",
    "    if len(buffer) >= minLen or buffer == [\"AH\"] or buffer == ['AY']:\n",
    "        segments.append(buffer)\n",
    "\n",
    "    decodedWords = []\n",
    "    normDistTotal = 0.0\n",
    "    for segment in segments:\n",
    "        matches = fuzzyMatch(segment, lexicon, maxDist = int(len(segment)) + 1)\n",
    "        if not matches:\n",
    "            decodedWords.append(\"<UNK>\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if decodedWords:\n",
    "            prefix = \" \".join(decodedWords)\n",
    "            prefixScore = lm.score(prefix, bos=True, eos=False)\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "            prefixScore = 0.0\n",
    "\n",
    "        scored = []\n",
    "        for word, normDist in matches:\n",
    "            if prefix:\n",
    "                candSentence = prefix + \" \" + word\n",
    "            else:\n",
    "                candSentence = word\n",
    "            lmGain = lm.score(candSentence, bos=True, eos=False) - prefixScore\n",
    "\n",
    "            score = lambdaLM * lmGain - lambdaDist * normDist\n",
    "            scored.append((word, score, normDist))\n",
    "\n",
    "        bestWord, bestScore, bestNormDist = max(scored, key=lambda x: x[1])\n",
    "        decodedWords.append(bestWord)\n",
    "        normDistTotal += bestNormDist\n",
    "\n",
    "    if decodedWords:\n",
    "        finalSentence = \" \".join(decodedWords)\n",
    "        finalLMScore = lm.score(finalSentence, bos=True, eos=True)\n",
    "    else:\n",
    "        finalSentence = \"\"\n",
    "        finalLMScore = 0.0\n",
    "\n",
    "    totalScore = lambdaLM * finalLMScore - lambdaDist * normDistTotal\n",
    "    return decodedWords, totalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "for i, phonemeSeq in enumerate(decodedOut):\n",
    "    words, _ = phoneme2wordsHardSegmentation(phonemeSeq, phonemizedWords, lm)\n",
    "    RESULTS.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER = []\n",
    "for i in range(len(RESULTS)):\n",
    "    error = wer(LABELS[9000 + i], RESULTS[i])\n",
    "    WER.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean word error rate:  0.796625507881853\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean word error rate: \", np.mean(WER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['IH', 'T', ' ', 'W', 'AA', 'Z', ' ', 'P', 'EY', 'T', ' ', 'F', 'AO', 'R']\n",
      "Ground truth phoneme sequence:  ['IH', 'T', ' ', 'W', 'AA', 'Z', ' ', 'P', 'EY', 'D', ' ', 'F', 'AO', 'R']\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  1\n",
      "Length of ground truth sequence:  14\n",
      " \n",
      "Decoded sentence:  it was eight for\n",
      "Original sentence:  it was paid for\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize WERs.\n",
    "\"\"\"\n",
    "which = 1381\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[9000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[9000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[9000 + which]))\n",
    "print(\" \")\n",
    "print(\"Decoded sentence: \", RESULTS[which])\n",
    "print(\"Original sentence: \", LABELS[9000 + which])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
