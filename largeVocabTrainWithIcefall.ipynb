{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnn\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof for table 1, figure 2, and figure 3. Icefall code is from https://github.com/k2-fsa/icefall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train LARGE-VOCAB EMG-to-phoneme conversion.\n",
    "\n",
    "For description of the data, please see largeVocabDataVisualization.ipynb\n",
    "\n",
    "Unlike data SMALL-VOCAB, there are no timestamps between words within a sentence. \n",
    "\n",
    "Given a sentence, you decode it fully using CTC loss. The pipeline resembles standard speech-to-text (ASR) techniques.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"https://pypi.org/project/Levenshtein/ - install this Lev distance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Open Data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(\"DATA/dataLargeVocab.pkl\", \"rb\") as file:\n",
    "    DATA = pickle.load(file)\n",
    "\n",
    "with open(\"DATA/labelsLargeVocab.pkl\", \"rb\") as file:\n",
    "    LABELS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diag = TRUE or FALSE. Raw SPD matrices or approximately diagonalized?\"\"\"\n",
    "DIAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "English phoneme definitions.\n",
    "\"\"\"\n",
    "tok2id = {}\n",
    "with open(\"DATA/ckptsLargeVocab/lang_phone/tokens.txt\") as f:\n",
    "    for line in f:\n",
    "        s, i = line.strip().split()\n",
    "        i = int(i)\n",
    "        if s == \"<eps>\" or s.startswith(\"#\"):\n",
    "            continue\n",
    "        tok2id[s] = i\n",
    "PHONE_DEF = tok2id\n",
    "\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF[p]\n",
    "\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phonemize the sentences.\n",
    "\"\"\"\n",
    "\n",
    "phonemizedSentences = []\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    phones = []\n",
    "    for p in g2p(LABELS[i]): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p): \n",
    "            phones.append(p)\n",
    "    phonemizedSentences.append(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert phone-to-indices using look-up dictionary PHONE_DEF.\n",
    "\"\"\"\n",
    "\n",
    "phoneIndexedSentences = []\n",
    "for i in range(len(phonemizedSentences)):\n",
    "    current = phonemizedSentences[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phoneIndexedSentences.append(phoneID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenIdToClassIdx(tokenId: int) -> int:\n",
    "    return tokenId - 1   \n",
    "\n",
    "def phoneSeqToClassIdxSeq(phoneSeq):\n",
    "    return [tokenIdToClassIdx(PHONE_DEF[p]) for p in phoneSeq]\n",
    "\n",
    "classIndexedSentences = [phoneSeqToClassIdxSeq(seq) for seq in phonemizedSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pad the phone transcribed sentences to a common length (to be used with CTC loss).\n",
    "\"\"\"\n",
    "\n",
    "phonemizedLabels = np.zeros((len(classIndexedSentences), 76)) - 1\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    phonemizedLabels[i, 0:len(classIndexedSentences[i])] = classIndexedSentences[i]\n",
    "\n",
    "labelLengths = np.zeros((len(classIndexedSentences)))\n",
    "for i in range(len(classIndexedSentences)):\n",
    "    labelLengths[i] = len(classIndexedSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z-normalize the data along the time dimension.\n",
    "\"\"\"\n",
    "\n",
    "normDATA = []\n",
    "for i in range(len(DATA)):\n",
    "    Mean = np.mean(DATA[i], axis = -1)\n",
    "    Std = np.std(DATA[i], axis = -1)\n",
    "    normDATA.append((DATA[i] - Mean[..., np.newaxis])/Std[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Slice the matrices into 50ms segments with a step size of 20ms. Signal is sampled at 5000 Hertz.\n",
    "\"\"\"\n",
    "\n",
    "slicedMatrices = []\n",
    "for j in range(len(normDATA)):\n",
    "    collect = []\n",
    "    stepSize = 100 \n",
    "    windowSize = 125\n",
    "    dataLength = normDATA[j].shape[1]\n",
    "    numIters = (dataLength - windowSize) // stepSize + 1\n",
    "       \n",
    "    for i in range(numIters):\n",
    "        where = i * stepSize + windowSize\n",
    "        start = where - windowSize\n",
    "        End = where + windowSize\n",
    "        temp = 1/(2 * windowSize) * (normDATA[j][:, start:End] @ normDATA[j][:, start:End].T)\n",
    "        collect.append(0.9 * temp + 0.1 * np.trace(temp) * np.eye(31))\n",
    "    slicedMatrices.append(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Approximately diagonalize the matrices using Frechet mean. Use only TRAIN-VAL data for calculating Frechet mean.\n",
    "\"\"\"\n",
    "\n",
    "matricesForMean = []\n",
    "for i in range(9000):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        matricesForMean.append(slicedMatrices[i][j])\n",
    "\n",
    "matricesForMean = np.array(matricesForMean)\n",
    "manifoldMean = frechetMean()\n",
    "\n",
    "MEAN = manifoldMean.mean(matricesForMean.reshape(-1, 31, 31))\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "identityMatrix = np.eye(31)\n",
    "afterMatrices = np.tile(identityMatrix, (len(slicedMatrices), 409, 1, 1)) \n",
    "inputLengths = np.zeros((len(slicedMatrices)))\n",
    "for i in range(len(slicedMatrices)):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        if DIAG:\n",
    "            temp = eigenvectors.T @ slicedMatrices[i][j] @ eigenvectors\n",
    "        else:\n",
    "            temp = slicedMatrices[i][j]\n",
    "        afterMatrices[i, j] = temp\n",
    "    inputLengths[i] = len(slicedMatrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"np.save(\"DATA/ckptsLargeVocab/frechetMeanLargeVocab.npy\", MEAN)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, inputLength, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "        self.inputLength = inputLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.inputLength[index])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train-validation-test split.\n",
    "\"\"\"\n",
    "trainFeatures = afterMatrices[:8000]\n",
    "trainLabels = phonemizedLabels[:8000]\n",
    "trainLabelLengths = labelLengths[:8000]\n",
    "trainInputLengths = inputLengths[:8000]\n",
    "\n",
    "valFeatures = afterMatrices[8000:9000]\n",
    "valLabels = phonemizedLabels[8000:9000]\n",
    "valLabelLengths = labelLengths[8000:9000]\n",
    "valInputLengths = inputLengths[8000:9000]\n",
    "\n",
    "testFeatures = afterMatrices[9000:]\n",
    "testLabels = phonemizedLabels[9000:]\n",
    "testLabelLengths = labelLengths[9000:]\n",
    "testInputLengths = inputLengths[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainInputLengths, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valInputLengths, valLabelLengths)\n",
    "testDataset = BaseDataset(testFeatures, testLabels, testInputLengths, testLabelLengths)\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)\n",
    "testDataloader = DataLoader(testDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOperation(model,  device, trainLoader, rnnOptimizer, Loss):\n",
    "    model.train()\n",
    "    totalLoss = 0\n",
    "    for inputs, targets, inputLengths, targetLengths in trainLoader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "        \n",
    "        rnnOptimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, inputLengths.cpu())\n",
    "        loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "        loss.backward()\n",
    "        rnnOptimizer.step()\n",
    "\n",
    "        totalLoss += loss.item()\n",
    "        \n",
    "    \n",
    "    return totalLoss / len(trainLoader)\n",
    "\n",
    "\n",
    "def valOperation(model, device, valLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in valLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "\n",
    "    return totalLoss / len(valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348591\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To replicate the PER (phoneme error rate) for various model sizes and layers, change the variable here:\n",
    "euclideanRnn.RnnNet(41, modelHiddenDimension = 25, device, numLayers = 3).to(device)\n",
    "\"\"\"\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "model = euclideanRnn.RnnNet(41, 25, device, numLayers = 3).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 40, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Training loss: 3.6429, Val loss: 3.5203\n",
      "Epoch: 2/100, Training loss: 3.2046, Val loss: 2.7644\n",
      "Epoch: 3/100, Training loss: 2.4225, Val loss: 2.1230\n",
      "Epoch: 4/100, Training loss: 1.9946, Val loss: 1.8647\n",
      "Epoch: 5/100, Training loss: 1.8021, Val loss: 1.7321\n",
      "Epoch: 6/100, Training loss: 1.6886, Val loss: 1.6973\n",
      "Epoch: 7/100, Training loss: 1.6089, Val loss: 1.6418\n",
      "Epoch: 8/100, Training loss: 1.5483, Val loss: 1.6244\n",
      "Epoch: 9/100, Training loss: 1.5109, Val loss: 1.4869\n",
      "Epoch: 10/100, Training loss: 1.4661, Val loss: 1.5696\n",
      "Epoch: 11/100, Training loss: 1.4296, Val loss: 1.4996\n",
      "Epoch: 12/100, Training loss: 1.4009, Val loss: 1.4822\n",
      "Epoch: 13/100, Training loss: 1.3762, Val loss: 1.4684\n",
      "Epoch: 14/100, Training loss: 1.3521, Val loss: 1.4607\n",
      "Epoch: 15/100, Training loss: 1.3285, Val loss: 1.4174\n",
      "Epoch: 16/100, Training loss: 1.3026, Val loss: 1.4961\n",
      "Epoch: 17/100, Training loss: 1.2892, Val loss: 1.4244\n",
      "Epoch: 18/100, Training loss: 1.2725, Val loss: 1.3997\n",
      "Epoch: 19/100, Training loss: 1.2503, Val loss: 1.3554\n",
      "Epoch: 20/100, Training loss: 1.2260, Val loss: 1.3676\n",
      "Epoch: 21/100, Training loss: 1.2124, Val loss: 1.4028\n",
      "Epoch: 22/100, Training loss: 1.2037, Val loss: 1.3431\n",
      "Epoch: 23/100, Training loss: 1.1862, Val loss: 1.3857\n",
      "Epoch: 24/100, Training loss: 1.1803, Val loss: 1.3518\n",
      "Epoch: 25/100, Training loss: 1.1648, Val loss: 1.3203\n",
      "Epoch: 26/100, Training loss: 1.1553, Val loss: 1.3092\n",
      "Epoch: 27/100, Training loss: 1.1519, Val loss: 1.3729\n",
      "Epoch: 28/100, Training loss: 1.1321, Val loss: 1.3344\n",
      "Epoch: 29/100, Training loss: 1.1254, Val loss: 1.3483\n",
      "Epoch: 30/100, Training loss: 1.1118, Val loss: 1.2859\n",
      "Epoch: 31/100, Training loss: 1.1024, Val loss: 1.3661\n",
      "Epoch: 32/100, Training loss: 1.0907, Val loss: 1.3077\n",
      "Epoch: 33/100, Training loss: 1.0829, Val loss: 1.3439\n",
      "Epoch: 34/100, Training loss: 1.0786, Val loss: 1.3635\n",
      "Epoch: 35/100, Training loss: 1.0658, Val loss: 1.3274\n",
      "Epoch: 36/100, Training loss: 1.0707, Val loss: 1.3233\n",
      "Epoch: 37/100, Training loss: 1.0558, Val loss: 1.3842\n",
      "Epoch: 38/100, Training loss: 1.0424, Val loss: 1.3275\n",
      "Epoch: 39/100, Training loss: 1.0400, Val loss: 1.3401\n",
      "Epoch: 40/100, Training loss: 1.0418, Val loss: 1.2898\n",
      "Epoch: 41/100, Training loss: 1.0295, Val loss: 1.3342\n",
      "Epoch: 42/100, Training loss: 1.0168, Val loss: 1.2800\n",
      "Epoch: 43/100, Training loss: 1.0119, Val loss: 1.3031\n",
      "Epoch: 44/100, Training loss: 1.0103, Val loss: 1.2823\n",
      "Epoch: 45/100, Training loss: 1.0126, Val loss: 1.2743\n",
      "Epoch: 46/100, Training loss: 0.9981, Val loss: 1.2894\n",
      "Epoch: 47/100, Training loss: 0.9876, Val loss: 1.3070\n",
      "Epoch: 48/100, Training loss: 0.9827, Val loss: 1.3258\n",
      "Epoch: 49/100, Training loss: 0.9781, Val loss: 1.2824\n",
      "Epoch: 50/100, Training loss: 0.9702, Val loss: 1.2912\n",
      "Epoch: 51/100, Training loss: 0.9756, Val loss: 1.2663\n",
      "Epoch: 52/100, Training loss: 0.9692, Val loss: 1.2806\n",
      "Epoch: 53/100, Training loss: 0.9547, Val loss: 1.3141\n",
      "Epoch: 54/100, Training loss: 0.9602, Val loss: 1.3291\n",
      "Epoch: 55/100, Training loss: 0.9462, Val loss: 1.2754\n",
      "Epoch: 56/100, Training loss: 0.9463, Val loss: 1.3162\n",
      "Epoch: 57/100, Training loss: 0.9332, Val loss: 1.2677\n",
      "Epoch: 58/100, Training loss: 0.9337, Val loss: 1.2733\n",
      "Epoch: 59/100, Training loss: 0.9330, Val loss: 1.2500\n",
      "Epoch: 60/100, Training loss: 0.9287, Val loss: 1.2568\n",
      "Epoch: 61/100, Training loss: 0.9260, Val loss: 1.2562\n",
      "Epoch: 62/100, Training loss: 0.9235, Val loss: 1.2841\n",
      "Epoch: 63/100, Training loss: 0.9176, Val loss: 1.3222\n",
      "Epoch: 64/100, Training loss: 0.9100, Val loss: 1.2495\n",
      "Epoch: 65/100, Training loss: 0.8998, Val loss: 1.2696\n",
      "Epoch: 66/100, Training loss: 0.9117, Val loss: 1.2697\n",
      "Epoch: 67/100, Training loss: 0.8924, Val loss: 1.2983\n",
      "Epoch: 68/100, Training loss: 0.8927, Val loss: 1.2733\n",
      "Epoch: 69/100, Training loss: 0.8813, Val loss: 1.2634\n",
      "Epoch: 70/100, Training loss: 0.8871, Val loss: 1.2657\n",
      "Epoch: 71/100, Training loss: 0.8843, Val loss: 1.2693\n",
      "Epoch: 72/100, Training loss: 0.8729, Val loss: 1.2757\n",
      "Epoch: 73/100, Training loss: 0.8719, Val loss: 1.2821\n",
      "Epoch: 74/100, Training loss: 0.8737, Val loss: 1.2739\n",
      "Epoch: 75/100, Training loss: 0.8777, Val loss: 1.2822\n",
      "Epoch: 76/100, Training loss: 0.8755, Val loss: 1.2979\n",
      "Epoch: 77/100, Training loss: 0.8697, Val loss: 1.2464\n",
      "Epoch: 78/100, Training loss: 0.8601, Val loss: 1.3017\n",
      "Epoch: 79/100, Training loss: 0.8575, Val loss: 1.2823\n",
      "Epoch: 80/100, Training loss: 0.8519, Val loss: 1.3464\n",
      "Epoch: 81/100, Training loss: 0.8437, Val loss: 1.2281\n",
      "Epoch: 82/100, Training loss: 0.8548, Val loss: 1.2580\n",
      "Epoch: 83/100, Training loss: 0.8433, Val loss: 1.2690\n",
      "Epoch: 84/100, Training loss: 0.8407, Val loss: 1.2903\n",
      "Epoch: 85/100, Training loss: 0.8425, Val loss: 1.2513\n",
      "Epoch: 86/100, Training loss: 0.8339, Val loss: 1.2649\n",
      "Epoch: 87/100, Training loss: 0.8397, Val loss: 1.2598\n",
      "Epoch: 88/100, Training loss: 0.8277, Val loss: 1.3161\n",
      "Epoch: 89/100, Training loss: 0.8246, Val loss: 1.2510\n",
      "Epoch: 90/100, Training loss: 0.8249, Val loss: 1.3205\n",
      "Epoch: 91/100, Training loss: 0.8288, Val loss: 1.3153\n",
      "Epoch: 92/100, Training loss: 0.8299, Val loss: 1.3089\n",
      "Epoch: 93/100, Training loss: 0.8155, Val loss: 1.2978\n",
      "Epoch: 94/100, Training loss: 0.8173, Val loss: 1.3279\n",
      "Epoch: 95/100, Training loss: 0.8201, Val loss: 1.2278\n",
      "Epoch: 96/100, Training loss: 0.8104, Val loss: 1.3844\n",
      "Epoch: 97/100, Training loss: 0.8193, Val loss: 1.2872\n",
      "Epoch: 98/100, Training loss: 0.8131, Val loss: 1.3448\n",
      "Epoch: 99/100, Training loss: 0.8127, Val loss: 1.3062\n",
      "Epoch: 100/100, Training loss: 0.8017, Val loss: 1.2640\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Do training.\n",
    "\"\"\"\n",
    "\n",
    "valLOSS = []\n",
    "minLOSS = 100\n",
    "for epoch in range(numberEpochs):\n",
    "    trainLoss = trainOperation(model, device, trainDataloader, rnnOptimizer, lossFunction)\n",
    "    valLoss = valOperation(model, device, valDataloader, lossFunction)\n",
    "    valLOSS.append(valLoss)\n",
    "    if minLOSS > valLoss:\n",
    "        minLOSS = valLoss\n",
    "    torch.save(model.state_dict(), \"ckpts/largeVocab/\" + str(epoch) + \".pt\")\n",
    "    print(f'Epoch: {epoch + 1}/{numberEpochs}, Training loss: {trainLoss:.4f}, Val loss: {valLoss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ckpts/largeVocab/valLoss.npy\", valLOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2277567759156227\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "valLoss = np.load(\"ckpts/largeVocab/valLoss.npy\")\n",
    "print(np.min(valLoss))\n",
    "print(np.argmin(valLoss))\n",
    "epoch = np.argmin(valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOperation(model, device, testLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    Outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in testLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "\n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "            Outputs.append(outputs.transpose(0, 1))\n",
    "\n",
    "    return Outputs, totalLoss / len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple beam-search algorithm.\n",
    "\"\"\"\n",
    "\n",
    "def ctcPrefixBeamSearch(\n",
    "    logProbs,\n",
    "    testLen = None,\n",
    "    beamSize = 5,\n",
    "    blank = 40,\n",
    "    topk = None,\n",
    "    allowDoubles = True,\n",
    "):\n",
    "    \n",
    "    lp = np.asarray(logProbs)\n",
    "    Ttotal, V = lp.shape\n",
    "    T = Ttotal if testLen is None else int(min(testLen, Ttotal))\n",
    "\n",
    "    beams = {(): (0.0, -np.inf)}\n",
    "\n",
    "    def add(store, seq, addPb, addPnb):\n",
    "        if seq in store:\n",
    "            pb, pnb = store[seq]\n",
    "            if addPb  != -np.inf: pb  = np.logaddexp(pb,  addPb)\n",
    "            if addPnb != -np.inf: pnb = np.logaddexp(pnb, addPnb)\n",
    "            store[seq] = (pb, pnb)\n",
    "        else:\n",
    "            store[seq] = (addPb, addPnb)\n",
    "\n",
    "    for t in range(T):\n",
    "        row = lp[t] \n",
    "        new = {}\n",
    "\n",
    "        if topk is not None and topk < V:\n",
    "            cand = np.argpartition(row, -topk)[-topk:]\n",
    "            if blank not in cand:\n",
    "                worstIdx = cand[np.argmin(row[cand])]\n",
    "                cand[cand == worstIdx] = blank\n",
    "        else:\n",
    "            cand = range(V)\n",
    "\n",
    "        for seq, (pb, pnb) in beams.items():\n",
    "            add(new, seq, np.logaddexp(pb, pnb) + row[blank], -np.inf)\n",
    "\n",
    "            last = seq[-1] if seq else None\n",
    "\n",
    "            for c in cand:\n",
    "                if c == blank:\n",
    "                    continue\n",
    "                pC = row[c]\n",
    "\n",
    "                if c == last:\n",
    "            \n",
    "                    add(new, seq, -np.inf, pnb + pC)\n",
    "\n",
    "                    if allowDoubles:\n",
    "                        add(new, seq + (c,), -np.inf, pb + pC)\n",
    "                else:\n",
    "                    add(new, seq + (c,), -np.inf, np.logaddexp(pb, pnb) + pC)\n",
    "\n",
    "        if len(new) > beamSize:\n",
    "            items = sorted(new.items(),\n",
    "                           key = lambda kv: np.logaddexp(*kv[1]),\n",
    "                           reverse = True)[:beamSize]\n",
    "            beams = dict(items)\n",
    "        else:\n",
    "            beams = new\n",
    "\n",
    "    bestSeq = max(beams.items(), key = lambda kv: np.logaddexp(*kv[1]))[0]\n",
    "    return bestSeq\n",
    "\n",
    "def findClosestTranscription(decodedTranscript, phoneticTranscription):\n",
    "    \n",
    "    dist = Levenshtein.distance(decodedTranscript, phoneticTranscription)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS:  1.2180124750504127\n"
     ]
    }
   ],
   "source": [
    "#modelWeight = torch.load(\"DATA/ckptsLargeVocab/ckptWithoutSpaces.pt\", weights_only = True)\n",
    "modelWeight = torch.load(\"ckpts/largeVocab/\" + str(94)  + '.pt', weights_only = True)\n",
    "model.load_state_dict(modelWeight)\n",
    "output, testLoss = testOperation(model, device, testDataloader, lossFunction)\n",
    "\n",
    "print(\"TEST LOSS: \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for o in output:\n",
    "    for oo in o:\n",
    "        outs.append(oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1660\n",
      "torch.Size([225, 41])\n"
     ]
    }
   ],
   "source": [
    "print(len(outs))\n",
    "print(outs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONE_DEF1 = {}\n",
    "for k, v in PHONE_DEF.items():\n",
    "    PHONE_DEF1[v - 1] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVS = []\n",
    "decodedOut = []\n",
    "for i in range(1660):\n",
    "    decodedSymbols = ctcPrefixBeamSearch(outs[i].cpu().numpy(), testInputLengths[i]) \n",
    "    phoneOut = []\n",
    "    for i in range(len(decodedSymbols)):\n",
    "        phoneOut.append(PHONE_DEF1[decodedSymbols[i]])\n",
    "    decodedOut.append(phoneOut)\n",
    "\n",
    "levs = []\n",
    "phoneLENGTHS = []\n",
    "for i in range(len(decodedOut)):\n",
    "    phoneLENGTHS.append(len(phonemizedSentences[8000 + i]))\n",
    "    levs.append(findClosestTranscription(decodedOut[i], phonemizedSentences[8000 + i]))\n",
    "LEVS.append(np.mean(levs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sentences:  20.554819277108432\n",
      "Mean phoneme errors (insertion errors + deletion errors + substitution errors):  7.1168674698795185\n",
      "Percent phoneme error:  0.3462383869171478\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of sentences: \", np.mean(phoneLENGTHS))\n",
    "print(\"Mean phoneme errors (insertion errors + deletion errors + substitution errors): \", np.mean(levs))\n",
    "print(\"Percent phoneme error: \", np.sum(levs)/np.sum(phoneLENGTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 406   61 1070  364 1615 1599  346 1518  618   43  579  546  608  366\n",
      "  554 1575 1230 1064  277   34   80 1238 1511  225  685 1333  237  244\n",
      "   55  909  418 1341  378 1109  205 1494  932  584   87   15  143  878\n",
      "  349 1188 1458 1423 1459  433   81   16 1306 1539 1215  316  300 1469\n",
      " 1352 1272  220  289  363  983  821 1176 1637  748  732  548 1286 1082\n",
      "   62 1563 1020  721 1537 1513  504 1394 1271   84 1520  887  276 1071\n",
      " 1646 1209  120    6 1386 1261 1559  894 1510 1384 1438  929 1598  415\n",
      " 1295  238]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sort the decoded sentences from best-to-worst. Display 100 best decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['Y', 'UW', 'N', 'AA', 'K', 'P', 'IY', 'P', 'AH', 'L', 'D', 'AW', 'N']\n",
      "Ground truth phoneme sequence:  ['Y', 'UW', 'N', 'AA', 'K', 'P', 'IY', 'P', 'AH', 'L', 'D', 'AW', 'N']\n",
      "Ground truth label:  You knock people down.\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  0\n",
      "Length of ground truth sequence:  13\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "which = 406\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[8000 + which])\n",
    "print(\"Ground truth label: \", LABELS[8000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[8000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[8000 + which]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
