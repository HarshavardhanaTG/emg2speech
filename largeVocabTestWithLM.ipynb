{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "from basicOperations.manifoldOperations import matrixDistance, frechetMean\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "from rnn import euclideanRnn\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Proof for table 1, figure 2, and figure 3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train LARGE-VOCAB EMG-to-phoneme conversion.\n",
    "\n",
    "For description of the data, please see largeVocabDataVisualization.ipynb\n",
    "\n",
    "Unlike data SMALL-VOCAB, there are no timestamps between words within a sentence. \n",
    "\n",
    "Given a sentence, you decode it fully using CTC loss. The pipeline resembles standard speech-to-text (ASR) techniques.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"https://pypi.org/project/Levenshtein/ - install this Lev distance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Open Data.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"DATA/dataLargeVocab.pkl\", \"rb\") as file:\n",
    "    DATA = pickle.load(file)\n",
    "\n",
    "with open(\"DATA/labelsLargeVocab.pkl\", \"rb\") as file:\n",
    "    LABELS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "English phoneme definitions.\n",
    "\"\"\"\n",
    "\n",
    "PHONE_DEF = ['AO', 'OY', 'DH', 'ZH', 'SH', 'CH', 'UH', 'NG', 'IY', 'AA', 'W', 'S', 'IH', 'K', 'EY', 'JH', 'Y', 'N', 'OW', 'M', 'P', 'T', 'B', 'AY', 'UW', 'R', 'G', 'EH', 'Z', 'TH', 'AW', \n",
    "             'HH', 'AH', 'AE', 'L', 'ER', 'F', 'V', 'D', '<sp>', 'SIL']\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF.index(p)\n",
    "\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phonemize the sentences.\n",
    "\"\"\"\n",
    "\n",
    "phonemizedSentences = []\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    phones = []\n",
    "    for p in g2p(LABELS[i]): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p) or p == \" \": \n",
    "            if p == \" \":\n",
    "                phones.append(\"<sp>\")\n",
    "            else:\n",
    "                phones.append(p)\n",
    "    phonemizedSentences.append(phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert phone-to-indices using look-up dictionary PHONE_DEF.\n",
    "\"\"\"\n",
    "\n",
    "phoneIndexedSentences = []\n",
    "for i in range(len(phonemizedSentences)):\n",
    "    current = phonemizedSentences[i]\n",
    "    phoneID = []\n",
    "    for j in range(len(current)):\n",
    "        phoneID.append(phoneToId(current[j]))\n",
    "    phoneIndexedSentences.append(phoneID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pad the phone transcribed sentences to a common length (to be used with CTC loss).\n",
    "\"\"\"\n",
    "\n",
    "phonemizedLabels = np.zeros((len(phoneIndexedSentences), 76)) - 1\n",
    "for i in range(len(phoneIndexedSentences)):\n",
    "    phonemizedLabels[i, 0:len(phoneIndexedSentences[i])] = phoneIndexedSentences[i]\n",
    "\n",
    "labelLengths = np.zeros((len(phoneIndexedSentences)))\n",
    "for i in range(len(phoneIndexedSentences)):\n",
    "    labelLengths[i] = len(phoneIndexedSentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "z-normalize the data along the time dimension.\n",
    "\"\"\"\n",
    "\n",
    "normDATA = []\n",
    "for i in range(len(DATA)):\n",
    "    Mean = np.mean(DATA[i], axis = -1)\n",
    "    Std = np.std(DATA[i], axis = -1)\n",
    "    normDATA.append((DATA[i] - Mean[..., np.newaxis])/Std[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Slice the matrices into 50ms segments with a step size of 20ms.\n",
    "\"\"\"\n",
    "\n",
    "slicedMatrices = []\n",
    "for j in range(len(normDATA)):\n",
    "    collect = []\n",
    "    stepSize = 100 \n",
    "    windowSize = 125\n",
    "    dataLength = normDATA[j].shape[1]\n",
    "    numIters = (dataLength - windowSize) // stepSize + 1\n",
    "       \n",
    "    for i in range(numIters):\n",
    "        where = i * stepSize + windowSize\n",
    "        start = where - windowSize\n",
    "        End = where + windowSize\n",
    "        temp = 1/(2 * windowSize) * (normDATA[j][:, start:End] @ normDATA[j][:, start:End].T)\n",
    "        collect.append(0.9 * temp + 0.1 * np.trace(temp) * np.eye(31))\n",
    "    slicedMatrices.append(collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diag = TRUE or FALSE. Raw SPD matrices or approximately diagonalized?\"\"\"\n",
    "DIAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Approximately diagonalize the matrices using Frechet mean.\n",
    "\"\"\"\n",
    "\n",
    "MEAN = np.load(\"DATA/ckptsLargeVocab/frechetMeanLargeVocab.npy\")\n",
    "eigenvalues, eigenvectors = np.linalg.eig(MEAN)\n",
    "\n",
    "identityMatrix = np.eye(31)\n",
    "afterMatrices = np.tile(identityMatrix, (len(slicedMatrices), 409, 1, 1)) \n",
    "inputLengths = np.zeros((len(slicedMatrices)))\n",
    "for i in range(len(slicedMatrices)):\n",
    "    for j in range(len(slicedMatrices[i])):\n",
    "        if DIAG:\n",
    "            temp = eigenvectors.T @ slicedMatrices[i][j] @ eigenvectors\n",
    "        else: \n",
    "            temp = slicedMatrices[i][j]\n",
    "        afterMatrices[i, j] = temp\n",
    "    inputLengths[i] = len(slicedMatrices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels, inputLength, targetLength):\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.targetLength = targetLength\n",
    "        self.inputLength = inputLength\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputSeq = self.data[index].astype('float32')  \n",
    "        targetSeq = self.labels[index]\n",
    "        inputLength = int(self.inputLength[index])\n",
    "        targetLength = int(self.targetLength[index])\n",
    "        return inputSeq, targetSeq, inputLength, targetLength\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train-validation-test split.\n",
    "\"\"\"\n",
    "\n",
    "trainFeatures = afterMatrices[:8000]\n",
    "trainLabels = phonemizedLabels[:8000]\n",
    "trainLabelLengths = labelLengths[:8000]\n",
    "trainInputLengths = inputLengths[:8000]\n",
    "\n",
    "valFeatures = afterMatrices[8000:9000]\n",
    "valLabels = phonemizedLabels[8000:9000]\n",
    "valLabelLengths = labelLengths[8000:9000]\n",
    "valInputLengths = inputLengths[8000:9000]\n",
    "\n",
    "testFeatures = afterMatrices[9000:]\n",
    "testLabels = phonemizedLabels[9000:]\n",
    "testLabelLengths = labelLengths[9000:]\n",
    "testInputLengths = inputLengths[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels, trainInputLengths, trainLabelLengths)\n",
    "valDataset = BaseDataset(valFeatures, valLabels, valInputLengths, valLabelLengths)\n",
    "testDataset = BaseDataset(testFeatures, testLabels, testInputLengths, testLabelLengths)\n",
    "\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)\n",
    "testDataloader = DataLoader(testDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6348591\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To replicate the PER (phoneme error rate) for various model sizes and layers, change the variable here:\n",
    "euclideanRnn.RnnNet(41, modelHiddenDimension = 25, device, numLayers = 3).to(device)\n",
    "\"\"\"\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "numberEpochs = 100\n",
    "\n",
    "model = euclideanRnn.RnnNet(41, 25, device, numLayers = 3).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CTCLoss(blank = 40, zero_infinity = True)\n",
    "rnnOptimizer = optim.Adam(model.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOperation(model, device, testLoader, Loss):\n",
    "    model.eval()\n",
    "    totalLoss = 0\n",
    "    Outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, inputLengths, targetLengths in testLoader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputLengths, targetLengths = inputLengths.to(device), targetLengths.to(device)\n",
    "            \n",
    "            outputs = model(inputs, inputLengths.cpu()) \n",
    "\n",
    "            loss = Loss(outputs, targets, inputLengths, targetLengths)\n",
    "            totalLoss += loss.item()\n",
    "            Outputs.append(outputs.transpose(0, 1))\n",
    "\n",
    "    return Outputs, totalLoss / len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LOSS:  1.8337221510948674\n"
     ]
    }
   ],
   "source": [
    "modelWeight = torch.load(\"DATA/ckptsLargeVocab/ckptLargeVocab.pt\", weights_only = True)\n",
    "model.load_state_dict(modelWeight)\n",
    "output, testLoss = testOperation(model, device, testDataloader, lossFunction)\n",
    "\n",
    "print(\"TEST LOSS: \", testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for o in output:\n",
    "    for oo in o:\n",
    "        outs.append(oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctcBeamSearchPhoneLM(\n",
    "    logProbs, length, lm, phoneDef, beamSize = 20, lmWeight = 1.2,\n",
    "    insertionBonus = 0.0, topk = None, allowDoubles = True, blankPhone = \"SIL\"\n",
    "):\n",
    "    lp = np.asarray(logProbs)\n",
    "    Ttotal, V = lp.shape\n",
    "    T = Ttotal if length is None else int(min(length, Ttotal))\n",
    "    assert len(phoneDef) == V\n",
    "    LN10 = math.log(10.0)\n",
    "\n",
    "    blank = phoneDef.index(blankPhone)\n",
    "    idx2tok = [None if ph == blankPhone else ph for ph in phoneDef]\n",
    "\n",
    "    def lmBos():\n",
    "        if lm is None: return None\n",
    "        s = kenlm.State(); lm.BeginSentenceWrite(s); return s\n",
    "\n",
    "    def lmAdv(st, tok):\n",
    "        if lm is None or tok is None: return st, 0.0\n",
    "        if tok not in lm and \"<unk>\" in lm: tok = \"<unk>\"\n",
    "        if tok not in lm: return st, 0.0\n",
    "        ns = kenlm.State()\n",
    "        inc = lm.BaseScore(st, tok, ns) \n",
    "        return ns, inc * LN10\n",
    "\n",
    "    beams = {(): (0.0, -np.inf, lmBos())} \n",
    "\n",
    "    def add(store, seq, addPb, addPnb, st):\n",
    "        if seq in store:\n",
    "            pb, pnb, cur = store[seq]\n",
    "            if addPb  != -np.inf: pb  = np.logaddexp(pb,  addPb)\n",
    "            if addPnb != -np.inf: pnb = np.logaddexp(pnb, addPnb)\n",
    "            if cur is None and st is not None: cur = st\n",
    "            store[seq] = (pb, pnb, cur)\n",
    "        else:\n",
    "            store[seq] = (addPb, addPnb, st)\n",
    "\n",
    "    for t in range(T):\n",
    "        row = lp[t]\n",
    "        new = {}\n",
    "\n",
    "        if topk is not None and topk < V:\n",
    "            cand = np.argpartition(row, -topk)[-topk:]\n",
    "            if blank not in cand:\n",
    "                worst = cand[np.argmin(row[cand])]\n",
    "                cand[cand == worst] = blank\n",
    "        else:\n",
    "            cand = range(V)\n",
    "\n",
    "        for seq, (pb, pnb, st) in beams.items():\n",
    "            add(new, seq, np.logaddexp(pb, pnb) + row[blank], -np.inf, st)\n",
    "\n",
    "            last = seq[-1] if seq else None\n",
    "\n",
    "            for c in cand:\n",
    "                if c == blank: continue\n",
    "                pC = row[c]\n",
    "\n",
    "                if c == last:\n",
    "                    add(new, seq, -np.inf, pnb + pC, st)\n",
    "                    if allowDoubles:\n",
    "                        tok = idx2tok[c]\n",
    "                        ns, inc = lmAdv(st, tok)\n",
    "                        add(new, seq + (c,), -np.inf, pb + pC + lmWeight*inc + insertionBonus, ns)\n",
    "                else:\n",
    "                    tok = idx2tok[c]\n",
    "                    ns, inc = lmAdv(st, tok)\n",
    "                    add(new, seq + (c,), -np.inf, np.logaddexp(pb, pnb) + pC + lmWeight*inc + insertionBonus, ns)\n",
    "\n",
    "        if len(new) > beamSize:\n",
    "            items = sorted(new.items(),\n",
    "                           key=lambda kv: np.logaddexp(kv[1][0], kv[1][1]),\n",
    "                           reverse=True)[:beamSize]\n",
    "            beams = dict(items)\n",
    "        else:\n",
    "            beams = new\n",
    "\n",
    "    bestSeq = max(beams.items(), key=lambda kv: np.logaddexp(kv[1][0], kv[1][1]))[0]\n",
    "    return bestSeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"6gram.arpa is a phoneme level LM created using wiki-text-103 corpora. The entire corpora was rewritten on a phone level\n",
    "with <sp> denoting the blank space. \n",
    "\n",
    "bash: /mnt/dataDrive/kenLM/kenlm/build/bin/lmplz -o 6 --discount_fallback \\\n",
    "  < wikitext-103.phone.txt > 6gram.arpa\n",
    "\n",
    "Use this LM with beamsearch. This is very similar to https://github.com/facebookresearch/emg2qwerty.\n",
    "\n",
    "\n",
    "Then, we use a different LM for phone-to-word mapping.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /mnt/dataDrive/emgFullCorpora/toUpload/DATA/ckptsLargeVocab/6gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lm = kenlm.Model(\"DATA/ckptsLargeVocab/6gram.arpa\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVS = []\n",
    "decodedOut = []\n",
    "for i in range(1970):\n",
    "    decodedSymbols = ctcBeamSearchPhoneLM(\n",
    "    logProbs = outs[i].cpu().numpy(),    \n",
    "    length = int(testInputLengths[i]),\n",
    "    lm = lm,\n",
    "    phoneDef = PHONE_DEF,\n",
    "    beamSize = 5,\n",
    "    lmWeight = 0.4,\n",
    "    insertionBonus = 1.2,\n",
    "    topk = None,\n",
    "    allowDoubles = True,\n",
    "    blankPhone = \"SIL\",\n",
    ") \n",
    "    phoneOut = []\n",
    "    for i in range(len(decodedSymbols)):\n",
    "        phoneOut.append(PHONE_DEF[decodedSymbols[i]])\n",
    "    decodedOut.append(phoneOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestTranscription(decodedTranscript, phoneticTranscription):\n",
    "    \n",
    "    dist = Levenshtein.distance(decodedTranscript, phoneticTranscription)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "levs = []\n",
    "phoneLENGTHS = []\n",
    "for i in range(len(decodedOut)):\n",
    "    phoneLENGTHS.append(len(phonemizedSentences[9000 + i]))\n",
    "    levs.append(findClosestTranscription(decodedOut[i], phonemizedSentences[9000 + i]))\n",
    "LEVS.append(np.mean(levs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sentences:  24.543654822335025\n",
      "Mean phoneme error rate (insertion errors + deletion errors + substitution errors):  11.10253807106599\n",
      "Percent phoneme error:  0.45235879299290604\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of sentences: \", np.mean(phoneLENGTHS))\n",
    "print(\"Mean phoneme error rate (insertion errors + deletion errors + substitution errors): \", np.mean(levs))\n",
    "print(\"Percent phoneme error: \", np.mean(levs)/np.mean(phoneLENGTHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 939 1800   90 1771  540   89  835  155 1315  989 1817  881 1214 1901\n",
      " 1920 1212 1538  988 1374  170  536 1550  785  790  797 1222 1467  779\n",
      "  188 1128 1035 1382 1130 1504  839   42 1962 1027 1720  738 1908  602\n",
      "  693  841 1381  741 1119 1798 1564 1924 1447  197  371  173  815  289\n",
      " 1546 1231  778  740  894  674 1265  154  892  810 1026 1056 1385 1126\n",
      " 1116  685 1686 1473 1037 1807  354 1135  698  713 1961  706  476   78\n",
      "    8 1892 1244  620 1080  428 1185 1620 1619 1751  762  198  623 1121\n",
      " 1047  435]\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(np.array(levs)/np.array(phoneLENGTHS))\n",
    "print(indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['DH', 'AH', '<sp>', 'S', 'EY', 'M', '<sp>', 'S', 'IH', 'T', 'IY', '<sp>', 'AO', 'R', '<sp>', 'K', 'AW', 'N', 'T', 'IY']\n",
      "Ground truth phoneme sequence:  ['DH', 'AH', '<sp>', 'S', 'EY', 'M', '<sp>', 'S', 'IH', 'T', 'IY', '<sp>', 'AO', 'R', '<sp>', 'K', 'AW', 'N', 'T', 'IY']\n",
      "Ground truth label:  the same city or county\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  0\n",
      "Length of ground truth sequence:  20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize decoded sentences.\n",
    "\"\"\"\n",
    "\n",
    "which = 939\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[9000 + which])\n",
    "print(\"Ground truth label: \", LABELS[9000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[9000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[9000 + which]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do word modeling. It is a very simple 3-gram model.\n",
    "The model was trained on Librispeech-100 sentences with kneLM using the command: lmplz -o 3 < train.txt > 3gram.arpa\n",
    "\n",
    "1) We mainly rely on matching phoneme segments to word pronunciations in a lexicon dectionary containing all possible unique words.\n",
    "2) kneLM language model is only used to disambiguate and choose coherent word sequences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from itertools import product\n",
    "from heapq import heappush, heappop\n",
    "from jiwer import wer\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /mnt/dataDrive/emgFullCorpora/toUpload/DATA/ckptsLargeVocab/3gramLibri.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lm = kenlm.Model(\"DATA/ckptsLargeVocab/3gramLibri.arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/ckptsLargeVocab/libri100.txt\", \"r\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "\n",
    "uniqueWords = set()\n",
    "for line in lines:\n",
    "    words = re.findall(r\"\\b\\w+\\b\", line.lower())\n",
    "    uniqueWords.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34541\n"
     ]
    }
   ],
   "source": [
    "print(len(uniqueWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37663\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemizedWords = {}\n",
    "\n",
    "for word in sorted(uniqueWords):\n",
    "    phones = []\n",
    "    for p in g2p(word): \n",
    "        p = re.sub(r'[0-9]', '', p)   \n",
    "        if re.match(r'[A-Z]+', p) or p == \" \": \n",
    "            phones.append(p)\n",
    "    phonemizedWords[word] = phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzyMatch(\n",
    "    phonemeSegment: List[str],\n",
    "    lexicon: Dict[str, List[str]],\n",
    "    maxDist: int = 8,\n",
    "    topK: int = 20\n",
    ") -> List[Tuple[str, float]]:\n",
    "    matches = []\n",
    "    for word, phonemes in lexicon.items():\n",
    "        dist = Levenshtein.distance(phonemeSegment, phonemes)\n",
    "        if dist <= maxDist:\n",
    "            normDist = dist / max(len(phonemeSegment), len(phonemes))\n",
    "            matches.append((word, normDist))\n",
    "    return sorted(matches, key=lambda x: x[1])[:topK]\n",
    "\n",
    "def phoneme2wordsHardSegmentation(\n",
    "    phonemes: List[str],\n",
    "    lexicon: Dict[str, List[str]],\n",
    "    lm,\n",
    "    blankToken: str = '<sp>',\n",
    "    lambdaLM: float = 0.17,\n",
    "    lambdaDist: float = 0.83,\n",
    "    minLen: int = 2\n",
    ") -> Tuple[List[str], float]:\n",
    "   \n",
    "    segments = []\n",
    "    buffer = []\n",
    "    for p in phonemes:\n",
    "        if p == blankToken:\n",
    "            if len(buffer) >= minLen or buffer == [\"AH\"] or buffer == ['AY']:\n",
    "                segments.append(buffer)\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(p)\n",
    "    if len(buffer) >= minLen or buffer == [\"AH\"] or buffer == ['AY']:\n",
    "        segments.append(buffer)\n",
    "\n",
    "    decodedWords = []\n",
    "    normDistTotal = 0.0\n",
    "    for segment in segments:\n",
    "        matches = fuzzyMatch(segment, lexicon, maxDist = int(len(segment)) + 1)\n",
    "        if not matches:\n",
    "            decodedWords.append(\"<UNK>\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if decodedWords:\n",
    "            prefix = \" \".join(decodedWords)\n",
    "            prefixScore = lm.score(prefix, bos=True, eos=False)\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "            prefixScore = 0.0\n",
    "\n",
    "        scored = []\n",
    "        for word, normDist in matches:\n",
    "            if prefix:\n",
    "                candSentence = prefix + \" \" + word\n",
    "            else:\n",
    "                candSentence = word\n",
    "            lmGain = lm.score(candSentence, bos=True, eos=False) - prefixScore\n",
    "\n",
    "            score = lambdaLM * lmGain - lambdaDist * normDist\n",
    "            scored.append((word, score, normDist))\n",
    "\n",
    "        bestWord, bestScore, bestNormDist = max(scored, key=lambda x: x[1])\n",
    "        decodedWords.append(bestWord)\n",
    "        normDistTotal += bestNormDist\n",
    "\n",
    "    if decodedWords:\n",
    "        finalSentence = \" \".join(decodedWords)\n",
    "        finalLMScore = lm.score(finalSentence, bos=True, eos=True)\n",
    "    else:\n",
    "        finalSentence = \"\"\n",
    "        finalLMScore = 0.0\n",
    "\n",
    "    totalScore = lambdaLM * finalLMScore - lambdaDist * normDistTotal\n",
    "    return decodedWords, totalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "for i, phonemeSeq in enumerate(decodedOut):\n",
    "    words, _ = phoneme2wordsHardSegmentation(phonemeSeq, phonemizedWords, lm)\n",
    "    RESULTS.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER = []\n",
    "for i in range(len(RESULTS)):\n",
    "    error = wer(LABELS[9000 + i], RESULTS[i])\n",
    "    WER.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean word error rate:  0.7810801479963916\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean word error rate: \", np.mean(WER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded phoneme sequence:  ['DH', 'AH', '<sp>', 'S', 'EY', 'M', '<sp>', 'S', 'IH', 'T', 'IY', '<sp>', 'AO', 'R', '<sp>', 'K', 'AW', 'N', 'T', 'IY']\n",
      "Ground truth phoneme sequence:  ['DH', 'AH', '<sp>', 'S', 'EY', 'M', '<sp>', 'S', 'IH', 'T', 'IY', '<sp>', 'AO', 'R', '<sp>', 'K', 'AW', 'N', 'T', 'IY']\n",
      " \n",
      "Levenshtein distance between decoded and ground truth sequence:  0\n",
      "Length of ground truth sequence:  20\n",
      " \n",
      "Decoded sentence:  the same city or county\n",
      "Original sentence:  the same city or county\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualize WERs.\n",
    "\"\"\"\n",
    "which = 939\n",
    "print(\"Decoded phoneme sequence: \", decodedOut[which])\n",
    "print(\"Ground truth phoneme sequence: \", phonemizedSentences[9000 + which])\n",
    "print(\" \")\n",
    "print(\"Levenshtein distance between decoded and ground truth sequence: \", Levenshtein.distance(decodedOut[which], phonemizedSentences[9000 + which]))\n",
    "print(\"Length of ground truth sequence: \", len(phonemizedSentences[9000 + which]))\n",
    "print(\" \")\n",
    "print(\"Decoded sentence: \", RESULTS[which])\n",
    "print(\"Original sentence: \", LABELS[9000 + which])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
